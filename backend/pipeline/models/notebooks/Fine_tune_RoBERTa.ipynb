{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "premium"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JOUqYOxdw551"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "b2yazcoQPD3s",
    "outputId": "a256a5f3-b0af-4a9e-a80d-23b6f79f97b0"
   },
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'/device:GPU:0'"
      ],
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      }
     },
     "metadata": {},
     "execution_count": 2
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "LABELS = '/data/output_received_by.csv'\n",
    "TEXT = '/data/output_heavy.csv'\n",
    "\n",
    "nrows = None\n",
    "idx = None"
   ],
   "metadata": {
    "id": "nOfBV84lzs-a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "df_labels = pd.read_csv(PATH_TO_LABEL_TOP_100, dtype={'id': int, 'label': str, 'label_encoded': int}, sep=',')\n",
    "df_texts = pd.read_csv(PATH_TO_TEXT_NO_STOPWORDS, sep=',')\n",
    "\n",
    "df = pd.merge(df_texts, df_labels, on='id', how='left')\n",
    "df = df.fillna('')\n",
    "\n",
    "df = df[df['text'] != '']\n",
    "df = df[df['label'] != '']\n",
    "\n",
    "arr_texts = df.text.to_numpy()\n",
    "arr_labels = df.label_encoded.to_numpy()\n",
    "\n",
    "arr_labels_encoded_unique = np.unique(arr_labels)\n",
    "\n",
    "if nrows is not None:\n",
    "    idx = random.sample(range(0, len(arr_labels)), nrows)\n",
    "\n",
    "if idx is not None:\n",
    "    arr_labels = [arr_labels[i] for i in idx]\n",
    "    arr_texts = [arr_texts[i] for i in idx]\n",
    "\n",
    "arr_y = np.asarray(arr_labels)\n",
    "arr_x = np.asarray(arr_texts)"
   ],
   "metadata": {
    "id": "AGuN8ahJzNX9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "seed = 1337\n",
    "rng = np.random.RandomState(seed)\n",
    "rng.shuffle(arr_texts)\n",
    "rng = np.random.RandomState(seed)\n",
    "rng.shuffle(arr_y)\n",
    "\n",
    "# Extract a training & validation split\n",
    "validation_split = .1\n",
    "num_validation_samples = int(validation_split * len(arr_x))\n",
    "arr_x_train = arr_x[:-num_validation_samples]\n",
    "arr_x_test = arr_x[-num_validation_samples:]\n",
    "arr_y_train = arr_y[:-num_validation_samples]\n",
    "arr_y_test = arr_y[-num_validation_samples:]"
   ],
   "metadata": {
    "id": "lTjQAdw_0jlk"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, TFAutoModelForSequenceClassification, AutoConfig, BertModel, RobertaForSequenceClassification, TFRobertaForSequenceClassification, TFXLMRobertaForSequenceClassification\n",
    "from transformers import create_optimizer\n",
    "\n",
    "max_len = 512\n",
    "\n",
    "# config = AutoConfig.from_pretrained(\"distilroberta-base\")\n",
    "config = AutoConfig.from_pretrained(\"xlm-roberta-base\")\n",
    "config.hidden_dropout_prob = 0.15\n",
    "config.attention_probs_dropout_prob = 0.15\n",
    "config.num_labels = 100\n",
    "\n",
    "# model = TFXLMRobertaForSequenceClassification.from_pretrained(\"xlm-roberta-base\", config=config)\n",
    "# model = TFAutoModelForSequenceClassification.from_pretrained(\"distilroberta-base\", config=config)\n",
    "# model = TFXLMRobertaForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/Colab Notebooks/finetuned/\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "x_train = [e[:max_len] if isinstance(e, str) else '' for e in arr_x_train]\n",
    "x_test = [e[:max_len] if isinstance(e, str) else '' for e in arr_x_test]\n",
    "\n",
    "df = pd.DataFrame(np.stack((x_train, arr_y_train), axis=1), columns=['text', 'label'])\n",
    "df.to_csv('/content/train.csv', index=False)\n",
    "\n",
    "df = pd.DataFrame(np.stack((x_test, arr_y_test), axis=1), columns=['text', 'label'])\n",
    "df.to_csv('/content/test.csv', index=False)\n",
    "\n",
    "data_files = {\"train\": \"/content/train.csv\", \"test\": \"/content/test.csv\"}\n",
    "dataset = load_dataset(\"csv\", data_files=data_files)\n",
    "\n",
    "def preprocess_function(data):\n",
    "    return tokenizer(data['text'], truncation=False)\n",
    "\n",
    "tokenized_imdb_train = dataset['train'].map(preprocess_function, batched=True)\n",
    "tokenized_imdb_test = dataset['test'].map(preprocess_function, batched=True)\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\", max_length=max_len)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    tokenized_imdb_train,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_validation_set = model.prepare_tf_dataset(\n",
    "    tokenized_imdb_test,\n",
    "    shuffle=False,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "num_epochs = 10\n",
    "batches_per_epoch = len(tokenized_imdb_train) // batch_size\n",
    "total_train_steps = int(batches_per_epoch * num_epochs)\n",
    "optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=10000, num_train_steps=total_train_steps)\n",
    "# optimizer = optimizer=tf.keras.optimizers.Adam(learning_rate=5e-6, epsilon=1e-8)\n",
    "\n",
    "model.compile(optimizer=optimizer, metrics=['accuracy'])\n",
    "model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=num_epochs)\n",
    "\n",
    "model.save_pretrained('/content/drive/MyDrive/Colab Notebooks/XLM-RoBERTa', overwrite=True)"
   ],
   "metadata": {
    "id": "vovZSmQI2EwK"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
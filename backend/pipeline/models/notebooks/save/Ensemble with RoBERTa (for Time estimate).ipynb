{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "premium"
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ],
   "metadata": {
    "id": "-KXP4T44S0ZT"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JOUqYOxdw551"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install scikit-learn\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install tensorflow-gpu"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import TFBertModel, BertTokenizerFast, create_optimizer\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding, TFAutoModelForSequenceClassification, AutoConfig, BertModel, RobertaForSequenceClassification, TFRobertaForSequenceClassification, TFXLMRobertaForSequenceClassification\n",
    "from transformers import create_optimizer\n",
    "\n",
    "epochs = 1\n",
    "batch = 16\n",
    "\n",
    "nrows = None\n",
    "idx = None\n",
    "\n",
    "PATH_TO_LABEL_TIME = '/content/drive/MyDrive/Colab Notebooks/data/output_time.csv'\n",
    "PATH_TO_LABEL_TOP_100 = '/content/drive/MyDrive/Colab Notebooks/data/output.csv'\n",
    "\n",
    "PATH_TO_FEATURE_VECTOR = '/content/drive/MyDrive/Colab Notebooks/data/features/output.csv'\n",
    "PATH_TO_FEATURE_RECEIVED_BY = '/content/drive/MyDrive/Colab Notebooks/data/features/output_received_by.csv'\n",
    "PATH_TO_FEATURE_TITLE = '/content/drive/MyDrive/Colab Notebooks/data/features/output_title.csv'\n",
    "PATH_TO_FEATURE_OMK = '/content/drive/MyDrive/Colab Notebooks/data/features/output_omk.csv'\n",
    "PATH_TO_TEXT = '/content/drive/MyDrive/Colab Notebooks/data/output_heavy.csv'"
   ],
   "metadata": {
    "id": "b2yazcoQPD3s"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "def get_layer_for_texts(MAX_SEQUENCE_LENGTH=512):\n",
    "\n",
    "    base = TFXLMRobertaForSequenceClassification.from_pretrained(\"/content/drive/MyDrive/Colab Notebooks/models/XLM-RoBERTa\")\n",
    "\n",
    "    # Inputs for token indices and attention masks\n",
    "    input_ids = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='text.input_ids')\n",
    "    attention_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='text.attention_mask')\n",
    "\n",
    "    for layer in base.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # output = base([input_ids, attention_mask]).last_hidden_state[:, 0, :]\n",
    "    output = base([input_ids, attention_mask])[0]\n",
    "    output = tf.keras.layers.Dropout(rate=0.1)(output)\n",
    "\n",
    "\n",
    "    return input_ids, attention_mask, output\n",
    "\n",
    "def tokenize_texts(sentences, max_length=512, padding='max_length'):\n",
    "    return tokenizer(\n",
    "        sentences,\n",
    "        truncation=True,\n",
    "        padding=padding,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"tf\"\n",
    "    )\n",
    "\n",
    "def data(batch_size=batch):\n",
    "\n",
    "    # Load labels\n",
    "    df_time = pd.read_csv(PATH_TO_LABEL_TIME, dtype={'id': int, 'label': str, 'label_encoded': int}, sep=',', na_values='')\n",
    "    df_responsible = pd.read_csv(PATH_TO_LABEL_TOP_100, dtype={'id': int, 'label': str, 'label_encoded': int}, sep=',', na_values='')\n",
    "\n",
    "    # Load input features\n",
    "    df_feature_vector = pd.read_csv(PATH_TO_FEATURE_VECTOR, sep=',', na_values='')\n",
    "    df_feature_received_by = pd.read_csv(PATH_TO_FEATURE_RECEIVED_BY, sep=',', na_values='')\n",
    "    df_feature_title = pd.read_csv(PATH_TO_FEATURE_TITLE, sep=',', na_values='')\n",
    "    df_feature_omk = pd.read_csv(PATH_TO_FEATURE_OMK, sep=',', na_values='')\n",
    "\n",
    "    # Load input text\n",
    "    df_texts = pd.read_csv(PATH_TO_TEXT, sep=',')\n",
    "    df_texts = df_texts.rename(columns={'requestId': 'id'})\n",
    "    df_texts['text'] = df_texts['subject'] + \" \" + df_texts['description']\n",
    "\n",
    "    # Fill NaN just in case\n",
    "    df_time = df_time.fillna('')\n",
    "    df_responsible = df_responsible.fillna('')\n",
    "    df_feature_vector = df_feature_vector.fillna('')\n",
    "    df_feature_received_by = df_feature_received_by.fillna('')\n",
    "    df_feature_title = df_feature_title.fillna('')\n",
    "    df_feature_omk = df_feature_omk.fillna('')\n",
    "    df_texts = df_texts.fillna('')\n",
    "\n",
    "    # Make sure all ids are found in all sets\n",
    "    ids = list(\n",
    "        set(df_feature_vector.id.to_list()) &\n",
    "        set(df_feature_received_by.id.to_list()) &\n",
    "        set(df_feature_title.id.to_list()) &\n",
    "        set(df_feature_omk.id.to_list()) &\n",
    "        set(df_texts.id.to_list()) &\n",
    "        set(df_responsible.id.to_list()) &\n",
    "        set(df_time.id.to_list())\n",
    "    )\n",
    "    ids = np.unique(np.array(ids))\n",
    "\n",
    "    # Make sure all ids are found in all sets\n",
    "    df_time = df_time[df_time.id.isin(ids)]\n",
    "    df_responsible = df_responsible[df_responsible.id.isin(ids)]\n",
    "    df_feature_vector = df_feature_vector[df_feature_vector.id.isin(ids)]\n",
    "    df_feature_received_by = df_feature_received_by[df_feature_received_by.id.isin(ids)]\n",
    "    df_feature_title = df_feature_title[df_feature_title.id.isin(ids)]\n",
    "    df_feature_omk = df_feature_omk[df_feature_omk.id.isin(ids)]\n",
    "    df_texts = df_texts[df_texts.id.isin(ids)]\n",
    "\n",
    "    # Drop duplicates\n",
    "    df_time = df_time.drop_duplicates(subset='id')\n",
    "    df_responsible = df_responsible.drop_duplicates(subset='id')\n",
    "    df_feature_vector = df_feature_vector.drop_duplicates(subset='id')\n",
    "    df_feature_received_by = df_feature_received_by.drop_duplicates(subset='id')\n",
    "    df_feature_title = df_feature_title.drop_duplicates(subset='id')\n",
    "    df_feature_omk = df_feature_omk.drop_duplicates(subset='id')\n",
    "    df_texts = df_texts.drop_duplicates(subset='id')\n",
    "\n",
    "    # Sort by id\n",
    "    df_time = df_time.sort_values(by='id')\n",
    "    df_responsible = df_responsible.sort_values(by='id')\n",
    "    df_feature_vector = df_feature_vector.sort_values(by='id')\n",
    "    df_feature_received_by = df_feature_received_by.sort_values(by='id')\n",
    "    df_feature_title = df_feature_title.sort_values(by='id')\n",
    "    df_feature_omk = df_feature_omk.sort_values(by='id')\n",
    "    df_texts = df_texts.sort_values(by='id')\n",
    "\n",
    "    # Reset index for iteration purposes\n",
    "    df_time = df_time.reset_index(drop=True)\n",
    "    df_responsible = df_responsible.reset_index(drop=True)\n",
    "    df_feature_vector = df_feature_vector.reset_index(drop=True)\n",
    "    df_feature_received_by = df_feature_received_by.reset_index(drop=True)\n",
    "    df_feature_title = df_feature_title.reset_index(drop=True)\n",
    "    df_feature_omk = df_feature_omk.reset_index(drop=True)\n",
    "    df_texts = df_texts.reset_index(drop=True)\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    arr_time = np.random.rand(len(df_time), 100)\n",
    "    for iy, ix in np.ndindex(arr_time.shape):\n",
    "        arr_time[iy, ix] = 0 # (arr_time[iy, ix] / 2) + 0.5\n",
    "\n",
    "    for i, el in df_time.iterrows():\n",
    "        index = int(df_responsible.iloc[i].label_encoded)\n",
    "        if float(el.label) == 0.0:\n",
    "            arr_time[i][index] = 1.0\n",
    "        if float(el.label) == 1.0:\n",
    "            arr_time[i][index] = 2.0\n",
    "        if float(el.label) == 2.0:\n",
    "            arr_time[i][index] = 3.0\n",
    "        if float(el.label) == 3.0:\n",
    "            arr_time[i][index] = 4.0\n",
    "        if float(el.label) == 4.0:\n",
    "            arr_time[i][index] = 5.0\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    arr_responsible = df_responsible[['label_encoded']].to_numpy()\n",
    "    arr_texts = df_texts[['text']].to_numpy()\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    arr_feature_vector = df_feature_vector.drop(['id'], axis=1)\n",
    "    arr_feature_received_by = df_feature_received_by.drop(['id'], axis=1)\n",
    "    arr_feature_title = df_feature_title.drop(['id'], axis=1)\n",
    "    arr_feature_omk = df_feature_omk.drop(['id'], axis=1)\n",
    "\n",
    "    arr_feature_vector = arr_feature_vector.values\n",
    "    arr_feature_received_by = arr_feature_received_by.values\n",
    "    arr_feature_title = arr_feature_title.values\n",
    "    arr_feature_omk = arr_feature_omk.values\n",
    "\n",
    "    idx = None\n",
    "\n",
    "    if nrows is not None:\n",
    "        idx = random.sample(range(0, len(arr_feature_vector)), nrows)\n",
    "\n",
    "    if idx is not None:\n",
    "        arr_feature_vector = [arr_feature_vector[i] for i in idx]\n",
    "        arr_feature_received_by = [arr_feature_received_by[i] for i in idx]\n",
    "        arr_feature_title = [arr_feature_title[i] for i in idx]\n",
    "        arr_feature_omk = [arr_feature_omk[i] for i in idx]\n",
    "        arr_texts = [arr_texts[i] for i in idx]\n",
    "        arr_time = [arr_time[i] for i in idx]\n",
    "        arr_responsible = [arr_responsible[i] for i in idx]\n",
    "\n",
    "    arr_feature_vector = np.asarray(arr_feature_vector)\n",
    "    arr_feature_received_by = np.asarray(arr_feature_received_by)\n",
    "    arr_feature_title = np.asarray(arr_feature_title)\n",
    "    arr_feature_omk = np.asarray(arr_feature_omk)\n",
    "    arr_texts = np.asarray(arr_texts).reshape(-1)\n",
    "\n",
    "    arr_time = np.asarray(arr_time)\n",
    "    arr_responsible = np.asarray(arr_responsible)\n",
    "\n",
    "    train_feature_vector, \\\n",
    "    validation_feature_vector, \\\n",
    "    train_received_by, \\\n",
    "    validation_received_by, \\\n",
    "    train_title, \\\n",
    "    validation_title, \\\n",
    "    train_omk, \\\n",
    "    validation_omk, \\\n",
    "    train_texts, \\\n",
    "    validation_texts, \\\n",
    "    train_time, \\\n",
    "    validation_time, \\\n",
    "    train_responsible, \\\n",
    "    validation_responsible \\\n",
    "        = train_test_split(\n",
    "        arr_feature_vector.tolist(),\n",
    "        arr_feature_received_by.tolist(),\n",
    "        arr_feature_title.tolist(),\n",
    "        arr_feature_omk.tolist(),\n",
    "        arr_texts.tolist(),\n",
    "        arr_time.tolist(),\n",
    "        arr_responsible.tolist(),\n",
    "        test_size=.2,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    tokenized_train = defaultdict(list)\n",
    "    tokenized_train['feature_vector'] = tf.convert_to_tensor(train_feature_vector)\n",
    "    tokenized_train['feature_received_by'] = tf.convert_to_tensor(train_received_by)\n",
    "    tokenized_train['feature_title'] = tf.convert_to_tensor(train_title)\n",
    "    tokenized_train['feature_omk'] = tf.convert_to_tensor(train_omk)\n",
    "\n",
    "    tokenized_train['text.input_ids'] = dict(tokenize_texts(train_texts))['input_ids']\n",
    "    tokenized_train['text.attention_mask'] = dict(tokenize_texts(train_texts))['attention_mask']\n",
    "\n",
    "    tokenized_train_y = defaultdict(list)\n",
    "    tokenized_train_y['time'] = tf.convert_to_tensor(train_time)\n",
    "\n",
    "    tokenized_validation = defaultdict(list)\n",
    "    tokenized_validation['feature_vector'] = tf.convert_to_tensor(validation_feature_vector)\n",
    "    tokenized_validation['feature_received_by'] = tf.convert_to_tensor(validation_received_by)\n",
    "    tokenized_validation['feature_title'] = tf.convert_to_tensor(validation_title)\n",
    "    tokenized_validation['feature_omk'] = tf.convert_to_tensor(validation_omk)\n",
    "\n",
    "    tokenized_validation['text.input_ids'] = dict(tokenize_texts(validation_texts))['input_ids']\n",
    "    tokenized_validation['text.attention_mask'] = dict(tokenize_texts(validation_texts))['attention_mask']\n",
    "\n",
    "    tokenized_validation_y = defaultdict(list)\n",
    "    tokenized_validation_y['time'] = tf.convert_to_tensor(validation_time)\n",
    "\n",
    "    X = tf.data.Dataset.from_tensor_slices((\n",
    "        dict(tokenized_train),\n",
    "        dict(tokenized_train_y),\n",
    "    )).batch(batch_size).prefetch(1)\n",
    "\n",
    "    V = tf.data.Dataset.from_tensor_slices((\n",
    "        dict(tokenized_validation),\n",
    "        dict(tokenized_validation_y),\n",
    "    )).batch(batch_size).prefetch(1)\n",
    "\n",
    "    return X, V, \\\n",
    "           len(tokenized_train['feature_vector'][0]), \\\n",
    "           len(tokenized_train['feature_received_by'][0]), \\\n",
    "           len(tokenized_train['feature_title'][0]), \\\n",
    "           len(tokenized_train['feature_omk'][0]), \\\n",
    "           len(tokenized_train['text.input_ids'][0])"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "-yLXcYXvSWTF"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X, V, \\\n",
    "len_feature_vector, \\\n",
    "len_feature_received_by, \\\n",
    "len_feature_title, \\\n",
    "len_feature_omk, \\\n",
    "len_text, \\\n",
    "    = data()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "yElL8OLoa3b5"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "input_ids_texts, attention_mask_texts, output_texts = get_layer_for_texts()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "1gHgbsYQSWTI"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Input\n",
    "feature_vector_input = tf.keras.layers.Input(shape=(len_feature_vector,), dtype=tf.float64, name='feature_vector')\n",
    "feature_received_by_input = tf.keras.layers.Input(shape=(len_feature_received_by,), dtype=tf.float64, name='feature_received_by')\n",
    "feature_title_input = tf.keras.layers.Input(shape=(len_feature_title,), dtype=tf.float64, name='feature_title')\n",
    "feature_omk_input = tf.keras.layers.Input(shape=(len_feature_omk,), dtype=tf.float64, name='feature_omk')\n",
    "\n",
    "# Concatenate\n",
    "features_concat_time = tf.keras.layers.Concatenate()([\n",
    "    feature_vector_input,\n",
    "    feature_received_by_input,\n",
    "    feature_title_input,\n",
    "    feature_omk_input\n",
    "])\n",
    "\n",
    "output_time = tf.keras.layers.Dense(\n",
    "    units=len_feature_vector + len_feature_received_by + len_feature_title + len_feature_omk,\n",
    "    activation='softmax'\n",
    ")(features_concat_time)\n",
    "\n",
    "output_time = tf.keras.layers.Dropout(rate=.1)(output_time)\n",
    "\n",
    "output_time = tf.keras.layers.Dense(\n",
    "    units=len_feature_vector + len_feature_received_by + len_feature_title + len_feature_omk,\n",
    "    activation='softmax'\n",
    ")(output_time)\n",
    "\n",
    "output_time = tf.keras.layers.Dropout(rate=.1)(output_time)\n",
    "output_time = tf.keras.layers.Dense(units=100, activation='softmax')(output_time)\n",
    "\n",
    "output = tf.keras.layers.concatenate([output_time, output_texts])\n",
    "output = tf.keras.layers.Dense(units=100, activation='relu', name='time')(output)\n",
    "\n",
    "model = tf.keras.Model(\n",
    "    inputs=[\n",
    "        feature_vector_input,\n",
    "        feature_received_by_input,\n",
    "        feature_title_input,\n",
    "        feature_omk_input,\n",
    "        input_ids_texts,\n",
    "        attention_mask_texts\n",
    "    ],\n",
    "    outputs=[output],\n",
    ")"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "sRgOlhgza3b6"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batches_per_epoch = 100 // batch\n",
    "total_train_steps = int(batches_per_epoch * epochs)\n",
    "optimizer, _ = create_optimizer(init_lr=6e-5, num_warmup_steps=10000, num_train_steps=total_train_steps)\n",
    "\n",
    "class CustomAccuracy(tf.keras.losses.Loss):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def call(self, t, p):\n",
    "        # It worked with Adam @ lr = 3e-5 \n",
    "        return tf.reduce_mean(tf.math.square(tf.math.square(p * t) - t), axis=-1)\n",
    "\n",
    "model.compile(\n",
    "    loss=CustomAccuracy(),\n",
    "    optimizer=optimizer,\n",
    "    metrics=['mae'],\n",
    ")\n",
    "\n",
    "model.summary()"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "ugt8QJD9a3b6"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "\n",
    "model.fit(\n",
    "    x=X,\n",
    "    y=None,\n",
    "    epochs=epochs,\n",
    "    validation_data=V\n",
    ")"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "RzFbN6Bna3b7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model.save_weights('/content/drive/MyDrive/Colab Notebooks/weights/colab_06_11_1023/model_weight.h5')"
   ],
   "metadata": {
    "id": "5qIu7frjSReg"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
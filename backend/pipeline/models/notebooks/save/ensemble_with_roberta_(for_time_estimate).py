# -*- coding: utf-8 -*-
"""Ensemble with RoBERTa (for Time-estimate).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Bv6ub8nmejXdquzkzIoPALFLI5KgM0ry
"""

import os

import h5py as h5py
import pandas as pd
import numpy as np
import random
import tensorflow as tf

from collections import defaultdict
from sklearn.model_selection import train_test_split
from transformers import TFBertModel, BertTokenizerFast, create_optimizer
from transformers import AutoTokenizer, DataCollatorWithPadding, TFAutoModelForSequenceClassification, AutoConfig, BertModel, RobertaForSequenceClassification, TFRobertaForSequenceClassification, TFXLMRobertaForSequenceClassification
from transformers import create_optimizer

epochs = 1
batch = 32

nrows = 10000
idx = None

PATH_TO_LABEL_TIME = 'data/output_time.csv'
PATH_TO_LABEL_TOP_100 = 'data/output.csv'

PATH_TO_FEATURE_VECTOR = 'data/features/output.csv'
PATH_TO_FEATURE_RECEIVED_BY = 'data/features/output_received_by.csv'
PATH_TO_FEATURE_TITLE = 'data/features/output_title.csv'
PATH_TO_FEATURE_OMK = 'data/features/output_omk.csv'
PATH_TO_TEXT = 'data/output_heavy.csv'

tokenizer = AutoTokenizer.from_pretrained("xlm-roberta-base")

def get_layer_for_texts(MAX_SEQUENCE_LENGTH=512):

    base = TFXLMRobertaForSequenceClassification.from_pretrained("models/XLM-RoBERTa")

    # Inputs for token indices and attention masks
    input_ids = tf.keras.layers.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='text.input_ids')
    attention_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='text.attention_mask')

    for layer in base.layers:
        layer.trainable = False

    # output = base([input_ids, attention_mask]).last_hidden_state[:, 0, :]
    output = base([input_ids, attention_mask])[0]
    output = tf.keras.layers.Dropout(rate=0.1)(output)


    return input_ids, attention_mask, output

def tokenize_texts(sentences, max_length=512, padding='max_length'):
    return tokenizer(
        sentences,
        truncation=True,
        padding=padding,
        max_length=max_length,
        return_tensors="tf"
    )

def sample(
    train_feature_vector,
    train_received_by,
    train_title,
    train_omk,
    train_texts,
    train_time,
    train_responsible
):
    extra_train_feature_vector = []
    extra_train_received_by = []
    extra_train_title = []
    extra_train_omk = []
    extra_train_texts = []
    extra_train_time = []
    extra_train_responsible = []

    index_to_remove = []

    def delete_multiple_element(list_object, indices):
        indices = sorted(indices, reverse=True)
        for idx in indices:
            if idx < len(list_object):
                list_object.pop(idx)

    for e in list(range(100)):
        indices = [i for i, x in enumerate(train_responsible) if x == e]
        if len(indices) < 5000:
            for _ in list(range(5000 - len(indices))):
                index = random.choice(indices)
                extra_train_feature_vector.append(train_feature_vector[index])
                extra_train_received_by.append(train_received_by[index])
                extra_train_title.append(train_title[index])
                extra_train_omk.append(train_omk[index])
                extra_train_texts.append(train_texts[index])
                extra_train_time.append(train_time[index])
                extra_train_responsible.append(train_responsible[index])
        else:
            index_to_remove = index_to_remove + indices[-(len(indices) - 5000):]

    delete_multiple_element(train_feature_vector, index_to_remove)
    delete_multiple_element(train_received_by, index_to_remove)
    delete_multiple_element(train_title, index_to_remove)
    delete_multiple_element(train_omk, index_to_remove)
    delete_multiple_element(train_texts, index_to_remove)
    delete_multiple_element(train_time, index_to_remove)
    delete_multiple_element(train_responsible, index_to_remove)

    return \
        train_feature_vector + extra_train_feature_vector, \
        train_received_by + extra_train_received_by, \
        train_title + extra_train_title, \
        train_omk + extra_train_omk, \
        train_texts + extra_train_texts, \
        train_time + extra_train_time, \
        train_responsible + extra_train_responsible

def data(batch_size=batch):

    if not os.path.isfile('cache/arr_responsible.csv'):

        # Load labels
        df_time = pd.read_csv(PATH_TO_LABEL_TIME, dtype={'id': int, 'label': str, 'label_encoded': int}, sep=',', na_values='')
        df_responsible = pd.read_csv(PATH_TO_LABEL_TOP_100, dtype={'id': int, 'label': str, 'label_encoded': int}, sep=',', na_values='')

        # Load input features
        df_feature_vector = pd.read_csv(PATH_TO_FEATURE_VECTOR, sep=',', na_values='')
        df_feature_received_by = pd.read_csv(PATH_TO_FEATURE_RECEIVED_BY, sep=',', na_values='')
        df_feature_title = pd.read_csv(PATH_TO_FEATURE_TITLE, sep=',', na_values='')
        df_feature_omk = pd.read_csv(PATH_TO_FEATURE_OMK, sep=',', na_values='')

        # Load input text
        df_texts = pd.read_csv(PATH_TO_TEXT, sep=',')
        df_texts = df_texts.rename(columns={'requestId': 'id'})
        df_texts['text'] = df_texts['subject'] + " " + df_texts['description']

        # Fill NaN just in case
        df_time = df_time.fillna('')
        df_responsible = df_responsible.fillna('')
        df_feature_vector = df_feature_vector.fillna('')
        df_feature_received_by = df_feature_received_by.fillna('')
        df_feature_title = df_feature_title.fillna('')
        df_feature_omk = df_feature_omk.fillna('')
        df_texts = df_texts.fillna('')

        # Make sure all ids are found in all sets
        ids = list(
            set(df_feature_vector.id.to_list()) &
            set(df_feature_received_by.id.to_list()) &
            set(df_feature_title.id.to_list()) &
            set(df_feature_omk.id.to_list()) &
            set(df_texts.id.to_list()) &
            set(df_responsible.id.to_list()) &
            set(df_time.id.to_list())
        )
        ids = np.unique(np.array(ids))

        # Make sure all ids are found in all sets
        df_time = df_time[df_time.id.isin(ids)]
        df_responsible = df_responsible[df_responsible.id.isin(ids)]
        df_feature_vector = df_feature_vector[df_feature_vector.id.isin(ids)]
        df_feature_received_by = df_feature_received_by[df_feature_received_by.id.isin(ids)]
        df_feature_title = df_feature_title[df_feature_title.id.isin(ids)]
        df_feature_omk = df_feature_omk[df_feature_omk.id.isin(ids)]
        df_texts = df_texts[df_texts.id.isin(ids)]

        # Drop duplicates
        df_time = df_time.drop_duplicates(subset='id')
        df_responsible = df_responsible.drop_duplicates(subset='id')
        df_feature_vector = df_feature_vector.drop_duplicates(subset='id')
        df_feature_received_by = df_feature_received_by.drop_duplicates(subset='id')
        df_feature_title = df_feature_title.drop_duplicates(subset='id')
        df_feature_omk = df_feature_omk.drop_duplicates(subset='id')
        df_texts = df_texts.drop_duplicates(subset='id')

        # Sort by id
        df_time = df_time.sort_values(by='id')
        df_responsible = df_responsible.sort_values(by='id')
        df_feature_vector = df_feature_vector.sort_values(by='id')
        df_feature_received_by = df_feature_received_by.sort_values(by='id')
        df_feature_title = df_feature_title.sort_values(by='id')
        df_feature_omk = df_feature_omk.sort_values(by='id')
        df_texts = df_texts.sort_values(by='id')

        # Reset index for iteration purposes
        df_time = df_time.reset_index(drop=True)
        df_responsible = df_responsible.reset_index(drop=True)
        df_feature_vector = df_feature_vector.reset_index(drop=True)
        df_feature_received_by = df_feature_received_by.reset_index(drop=True)
        df_feature_title = df_feature_title.reset_index(drop=True)
        df_feature_omk = df_feature_omk.reset_index(drop=True)
        df_texts = df_texts.reset_index(drop=True)

        # ----------------------------------------------------------------------------------------------------------------------

        arr_time = np.random.rand(len(df_time), 100)
        for iy, ix in np.ndindex(arr_time.shape):
            arr_time[iy, ix] = 0 # (arr_time[iy, ix] / 2) + 0.5

        for i, el in df_time.iterrows():
            index = int(df_responsible.iloc[i].label_encoded)
            if float(el.label) == 0.0:
                arr_time[i][index] = 1.0
            if float(el.label) == 1.0:
                arr_time[i][index] = 2.0
            if float(el.label) == 2.0:
                arr_time[i][index] = 3.0
            if float(el.label) == 3.0:
                arr_time[i][index] = 4.0
            if float(el.label) == 4.0:
                arr_time[i][index] = 5.0

        # ----------------------------------------------------------------------------------------------------------------------

        arr_responsible = df_responsible[['label_encoded']].to_numpy()
        arr_texts = df_texts[['text']].to_numpy()

        # ----------------------------------------------------------------------------------------------------------------------

        arr_feature_vector = df_feature_vector.drop(['id'], axis=1)
        arr_feature_received_by = df_feature_received_by.drop(['id'], axis=1)
        arr_feature_title = df_feature_title.drop(['id'], axis=1)
        arr_feature_omk = df_feature_omk.drop(['id'], axis=1)

        arr_feature_vector = arr_feature_vector.values
        arr_feature_received_by = arr_feature_received_by.values
        arr_feature_title = arr_feature_title.values
        arr_feature_omk = arr_feature_omk.values

        idx = None

        if nrows is not None:
            idx = random.sample(range(0, len(arr_feature_vector)), nrows)

        if idx is not None:
            arr_feature_vector = [arr_feature_vector[i] for i in idx]
            arr_feature_received_by = [arr_feature_received_by[i] for i in idx]
            arr_feature_title = [arr_feature_title[i] for i in idx]
            arr_feature_omk = [arr_feature_omk[i] for i in idx]
            arr_texts = [arr_texts[i] for i in idx]
            arr_time = [arr_time[i] for i in idx]
            arr_responsible = [arr_responsible[i] for i in idx]

        arr_feature_vector = np.asarray(arr_feature_vector)
        arr_feature_received_by = np.asarray(arr_feature_received_by)
        arr_feature_title = np.asarray(arr_feature_title)
        arr_feature_omk = np.asarray(arr_feature_omk)
        arr_texts = np.asarray(arr_texts).reshape(-1)

        arr_time = np.asarray(arr_time)
        arr_responsible = np.asarray(arr_responsible).reshape(-1)

        pd.DataFrame(arr_feature_vector).to_csv('cache/arr_feature_vector.csv', index_label=False)
        pd.DataFrame(arr_feature_received_by).to_csv('cache/arr_feature_received_by.csv', index_label=False)
        pd.DataFrame(arr_feature_title).to_csv('cache/arr_feature_title.csv', index_label=False)
        pd.DataFrame(arr_feature_omk).to_csv('cache/arr_feature_omk.csv', index_label=False)
        pd.DataFrame(arr_texts).to_csv('cache/arr_texts.csv', index_label=False)
        pd.DataFrame(arr_time).to_csv('cache/arr_time.csv', index_label=False)
        pd.DataFrame(arr_responsible).to_csv('cache/arr_responsible.csv', index_label=False)
        print('Done.')

    arr_feature_vector = pd.read_csv('cache/arr_feature_vector.csv').values
    arr_feature_received_by = pd.read_csv('cache/arr_feature_received_by.csv').values
    arr_feature_title = pd.read_csv('cache/arr_feature_title.csv').values
    arr_feature_omk = pd.read_csv('cache/arr_feature_omk.csv').values
    arr_texts = pd.read_csv('cache/arr_texts.csv').fillna('').values.reshape(-1)
    arr_time = pd.read_csv('cache/arr_time.csv').values
    arr_responsible = pd.read_csv('cache/arr_responsible.csv').values.reshape(-1)

    train_feature_vector, \
    validation_feature_vector, \
    train_received_by, \
    validation_received_by, \
    train_title, \
    validation_title, \
    train_omk, \
    validation_omk, \
    train_texts, \
    validation_texts, \
    train_time, \
    validation_time, \
    train_responsible, \
    validation_responsible \
        = train_test_split(
        arr_feature_vector.tolist(),
        arr_feature_received_by.tolist(),
        arr_feature_title.tolist(),
        arr_feature_omk.tolist(),
        arr_texts.tolist(),
        arr_time.tolist(),
        arr_responsible.tolist(),
        test_size=.2,
        shuffle=True
    )

    train_feature_vector, \
    train_received_by,\
    train_title, \
    train_omk, \
    train_texts, \
    train_time, \
    train_responsible \
        = sample(
        train_feature_vector,
        train_received_by,
        train_title,
        train_omk,
        train_texts,
        train_time,
        train_responsible
    )

    tokenized_train = defaultdict(list)
    tokenized_train['feature_vector'] = tf.convert_to_tensor(train_feature_vector)
    tokenized_train['feature_received_by'] = tf.convert_to_tensor(train_received_by)
    tokenized_train['feature_title'] = tf.convert_to_tensor(train_title)
    tokenized_train['feature_omk'] = tf.convert_to_tensor(train_omk)

    # tokenized_train['text.input_ids'] = dict(tokenize_texts(train_texts))['input_ids']
    # tokenized_train['text.attention_mask'] = dict(tokenize_texts(train_texts))['attention_mask']

    tokenized_train_y = defaultdict(list)
    tokenized_train_y['time'] = tf.convert_to_tensor(train_time)
    # tokenized_train_y['responsible'] = tf.convert_to_tensor(train_responsible)

    tokenized_validation = defaultdict(list)
    tokenized_validation['feature_vector'] = tf.convert_to_tensor(validation_feature_vector)
    tokenized_validation['feature_received_by'] = tf.convert_to_tensor(validation_received_by)
    tokenized_validation['feature_title'] = tf.convert_to_tensor(validation_title)
    tokenized_validation['feature_omk'] = tf.convert_to_tensor(validation_omk)

    # tokenized_validation['text.input_ids'] = dict(tokenize_texts(validation_texts))['input_ids']
    # tokenized_validation['text.attention_mask'] = dict(tokenize_texts(validation_texts))['attention_mask']

    tokenized_validation_y = defaultdict(list)
    tokenized_validation_y['time'] = tf.convert_to_tensor(validation_time)
    # tokenized_validation_y['responsible'] = tf.convert_to_tensor(validation_responsible)

    X = tf.data.Dataset.from_tensor_slices((
        dict(tokenized_train),
        dict(tokenized_train_y),
    )).batch(batch_size).prefetch(1)

    V = tf.data.Dataset.from_tensor_slices((
        dict(tokenized_validation),
        dict(tokenized_validation_y),
    )).batch(batch_size).prefetch(1)

    return X, V, \
        len(tokenized_train['feature_vector'][0]), \
        len(tokenized_train['feature_received_by'][0]), \
        len(tokenized_train['feature_title'][0]), \
        len(tokenized_train['feature_omk'][0]), \
        len(tokenized_train['text.input_ids'][0])

def traverse_datasets(hdf_file):

    def h5py_dataset_iterator(g, prefix=''):
        for key in g.keys():
            item = g[key]
            path = f'{prefix}/{key}'
            if isinstance(item, h5py.Dataset):
                yield (path, item)
            elif isinstance(item, h5py.Group):
                yield from h5py_dataset_iterator(item, path)

    for path, _ in h5py_dataset_iterator(hdf_file):
        yield path

"""
for dataset in traverse_datasets(hf):
    print('Path:', dataset)
    print(hf[dataset])
    print('-----------------------')

exit(1)
"""

X, V, \
len_feature_vector, \
len_feature_received_by, \
len_feature_title, \
len_feature_omk, \
len_text, \
    = data()

"""
for el in [
    'feature_vector',
    'feature_received_by',
    'feature_title',
    'feature_omk',
]:
"""

# input_ids_texts, attention_mask_texts, output_texts = get_layer_for_texts()

# Input
# feature_vector_input = tf.keras.layers.Input(shape=(len_feature_vector,), dtype=tf.float64, name='feature_vector')
feature_received_by_input = tf.keras.layers.Input(shape=(len_feature_received_by,), dtype=tf.float64, name='feature_received_by')
# feature_title_input = tf.keras.layers.Input(shape=(len_feature_title,), dtype=tf.float64, name='feature_title')
# feature_omk_input = tf.keras.layers.Input(shape=(len_feature_omk,), dtype=tf.float64, name='feature_omk')

# Concatenate
# features_concat_time = tf.keras.layers.Concatenate()([feature_vector_input, feature_received_by_input, feature_title_input, feature_omk_input])

# output_time = tf.keras.layers.Dense(units=len_feature_vector + len_feature_received_by + len_feature_title + len_feature_omk, activation='softmax')(features_concat_time)

# output_time = tf.keras.layers.Dense(units=len_feature_received_by, activation='softmax')(feature_received_by_input)
# output_time = tf.keras.layers.Dropout(rate=.1)(output_time)

# output_time = tf.keras.layers.Dense(units=100, activation='softmax')(output_time)

# output = tf.keras.layers.concatenate([output_time, output_texts])
output = tf.keras.layers.Dense(units=100, activation='softmax', name='time')(feature_received_by_input)

model = tf.keras.Model(
    inputs=[
        # feature_vector_input,
        feature_received_by_input,
        # feature_title_input,
        # feature_omk_input,
        # input_ids_texts,
        # attention_mask_texts
    ],
    outputs=[output],
)

batches_per_epoch = 100 // batch
total_train_steps = int(batches_per_epoch * epochs)
optimizer, _ = create_optimizer(init_lr=6e-5, num_warmup_steps=10000, num_train_steps=total_train_steps)
# optimizer, _ = create_optimizer(init_lr=6e-5, num_warmup_steps=10000, num_train_steps=total_train_steps)

class CustomAccuracy(tf.keras.losses.Loss):

    def __init__(self):
        super().__init__()

    def call(self, t, p):
        # It worked with Adam @ lr = 3e-5 
        # return tf.reduce_mean(tf.math.square(tf.math.square(p * t) - t), axis=-1)
        # return tf.reduce_mean(tf.math.square(tf.math.pow((p - t) * t, 2)), axis=-1)
        return tf.reduce_mean(tf.math.square(tf.math.pow((p - t) * tf.math.log(tf.math.log(t + 1) + 1), 2)), axis=-1)

model.compile(
    # loss=CustomAccuracy(),
    # optimizer=optimizer,
    # loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),
    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=False),
    optimizer=tf.keras.optimizers.SGD(learning_rate=0.005),
    metrics=['mae'],
)

model.summary()
print(model.layers)

version = 'colab_13_11_1541'
version_load = '1541'
version_save = '1541'

if False:
    filename = f'weights/{version}/model_weight_{version_load}.h5'
    with h5py.File(filename) as hf:
        w = [tf.constant(hf['/dense/dense/kernel:0']), tf.constant(hf['/dense/dense/bias:0'])]
        model.get_layer('dense').set_weights(w)
        w = [tf.constant(hf['/responsible/responsible/kernel:0']), tf.constant(hf['/responsible/responsible/bias:0'])]
        model.get_layer('responsible').set_weights(w)

epochs = 100

# model.load_weights('weights/colab_07_11_2028/model_weight.h5')

model.fit(
    x=X,
    y=None,
    epochs=epochs,
    validation_data=V,
    # callbacks=[tf.keras.callbacks.LearningRateScheduler(lambda epoch: 1e-3 * 10 ** (epoch / 25))]
)

model.save_weights(f'weights/{version}/model_weight_{version_save}.h5')

validation = np.concatenate([y['time'] for x, y in V])
prediction = model.predict(V)

pd.DataFrame(validation).to_csv(f'predict/{version}/{version_save}_true.csv', index_label=False)
pd.DataFrame(prediction).to_csv(f'predict/{version}/{version_save}_prediction.csv', index_label=False)
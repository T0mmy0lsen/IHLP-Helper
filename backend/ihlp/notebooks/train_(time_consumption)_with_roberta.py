# -*- coding: utf-8 -*-
"""Train (time-consumption) with RoBERTa.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15_VNFIBh4W9f_5Ox3hUO_B3Ujm62FBNT

### 0. Connect to Google drive (if in Colab)
"""

"""### 2. Set som Global"""

import os.path

LABEL = 'label_timeconsumption'
LABEL_MODEL = 'Time-Consumption'
LABEL_COUNT = 1
MAX_SIZE = 512
BATCH = 32
EPOCHS = 8
PATH_RELATIVE = 'data/'
HAS_CACHE_DATA = os.path.isfile(PATH_RELATIVE + f'cached_train_{LABEL}.csv')

"""### 3. Load the data"""

if not HAS_CACHE_DATA:

    from sklearn.model_selection import train_test_split
    import pandas as pd

    df_labels = pd.read_csv(
        filepath_or_buffer=PATH_RELATIVE + f'{LABEL}.csv',
        dtype={'id': int, 'label_encoded': int},
        sep=','
    )

    df_subject = pd.read_csv(
        filepath_or_buffer=PATH_RELATIVE + f'subject.csv',
        sep=','
    )

    df_description = pd.read_csv(
        filepath_or_buffer=PATH_RELATIVE + f'description.csv',
        sep=','
    )

    df_texts = pd.merge(df_subject, df_description, on='id', how='inner')
    df_texts = df_texts.fillna('')
    df_texts['text'] = df_texts.apply(lambda x:  x['subject'] + ". " + x['description'], axis=1)

"""### 4. Merge and renaming"""

if not HAS_CACHE_DATA:

    df = pd.merge(df_texts, df_labels, on='id', how='inner')
    df = df[['text', 'label_encoded']]
    df = df.rename(columns={'label_encoded': 'label'})
    df = df.fillna('')

    df = df[df['text'] != '']
    df['text'] = df.apply(lambda x: x['text'][:MAX_SIZE], axis=1)

"""### 5. Split the data"""

if not HAS_CACHE_DATA:

    train, test = train_test_split(df, test_size=0.2, random_state=1)
    train.to_csv(PATH_RELATIVE + f'cached_train_{LABEL}.csv', index=False)
    test.to_csv(PATH_RELATIVE + f'cached_test_{LABEL}.csv', index=False)

"""### 6. Setting up the configuration for the Transformer"""

from transformers import AutoConfig

config = AutoConfig.from_pretrained("xlm-roberta-base")
config.hidden_dropout_prob = 0.125
config.attention_probs_dropout_prob = 0.125
config.num_labels = LABEL_COUNT

"""### 7. Loading in the model and tokenizer
The `from_pretrained` parameter can simply be changed to the saved output folder if you wan't to
continue training an already fine-tuned model.
"""

from transformers import TFAutoModel, AutoTokenizer

model = TFAutoModel.from_pretrained("xlm-roberta-base", config=config)
tokenizer = AutoTokenizer.from_pretrained("xlm-roberta-base")

"""### 8. Change the input data into a TF dataset"""

from datasets import load_dataset
from transformers import DataCollatorWithPadding

data_files = {'train': PATH_RELATIVE + f'cached_train_{LABEL}.csv', 'test': PATH_RELATIVE + f'cached_test_{LABEL}.csv'}
dataset = load_dataset("csv", data_files=data_files)

def preprocess_function(data):
    return tokenizer(data['text'], truncation=False)

tokenized_train = dataset['train'].map(preprocess_function, batched=True)
tokenized_test = dataset['test'].map(preprocess_function, batched=True)

data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf", max_length=512)

tf_train_set = model.prepare_tf_dataset(
    tokenized_train,
    shuffle=True,
    batch_size=BATCH,
    collate_fn=data_collator,
)

tf_validation_set = model.prepare_tf_dataset(
    tokenized_test,
    shuffle=False,
    batch_size=BATCH,
    collate_fn=data_collator,
)

"""### 9. Compile the model"""

from transformers import create_optimizer
import tensorflow as tf

CONTINUE_TRAINING = False

if CONTINUE_TRAINING:
    EPOCHS = 2
    model = TFAutoModel.from_pretrained(
        PATH_RELATIVE + f'models/IHLP-XLM-RoBERTa-{LABEL_MODEL}', config=config
    )

batches_per_epoch = len(tokenized_train) // BATCH
total_train_steps = int(batches_per_epoch * EPOCHS)
optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=10000, num_train_steps=total_train_steps)

base_model = TFAutoModel.from_pretrained("xlm-roberta-base", config=config)

# Define the input layers
input_ids = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name="input_ids")
attention_mask = tf.keras.layers.Input(shape=(None,), dtype=tf.int32, name="attention_mask")

# Get the base model's output
outputs_base_model = base_model(input_ids, attention_mask=attention_mask)

# Add a dense layer for regression
regression_head = tf.keras.layers.Dense(1, activation=None)(outputs_base_model[0][:, 0, :])

# Define the final model
model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=regression_head)


model.compile(optimizer=optimizer, loss=tf.keras.losses.MeanSquaredError())

"""### 10. Train the model"""

model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=EPOCHS)

"""### 11. Save the model"""

model.save_pretrained(PATH_RELATIVE + f'models/IHLP-XLM-RoBERTa-{LABEL_MODEL}', overwrite=True)